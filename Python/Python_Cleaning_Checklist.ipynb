{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c90b2",
   "metadata": {},
   "source": [
    "## Cleaning Checklist \n",
    "Note: argument \"inplace=True\" will modify original dataframe. inplace = False [Default] will generate a seperate dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8051fa71",
   "metadata": {},
   "source": [
    "1. Profiling\n",
    "2. Missing Data\n",
    "3. Duplicate Data\n",
    "4. Correct Data Types\n",
    "5. Numeric Data\n",
    "6. Text Data\n",
    "7. Date and Time Data\n",
    "8. Categorical Data\n",
    "9. Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab495b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02eb955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64122c",
   "metadata": {},
   "source": [
    "Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "    age   sex   trx  week  wbc  rbc adverse_effects  num_effects\n",
      "0   62  male  Drug     0  7.3  5.1              No            0\n",
      "1   62  male  Drug     1  NaN  NaN              No            0\n",
      "2   62  male  Drug    12  5.6  5.0              No            0\n",
      "3   62  male  Drug    16  NaN  NaN              No            0\n",
      "4   62  male  Drug     2  6.6  5.1              No            0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16103 entries, 0 to 16102\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              16103 non-null  int64  \n",
      " 1   sex              16103 non-null  object \n",
      " 2   trx              16103 non-null  object \n",
      " 3   week             16103 non-null  int64  \n",
      " 4   wbc              9128 non-null   float64\n",
      " 5   rbc              9127 non-null   float64\n",
      " 6   adverse_effects  16103 non-null  object \n",
      " 7   num_effects      16103 non-null  int64  \n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 1006.6+ KB\n",
      "\n",
      "DataFrame Info:\n",
      " None\n",
      "\n",
      "Numerical Statistics:\n",
      "                 age         week          wbc          rbc   num_effects\n",
      "count  16103.000000  16103.00000  9128.000000  9127.000000  16103.000000\n",
      "mean      64.117556      7.74098     7.340557     4.672784      0.101596\n",
      "std        8.783207      6.94350     1.996652     0.458520      0.323181\n",
      "min       39.000000      0.00000     1.800000     2.100000      0.000000\n",
      "25%       58.000000      1.00000     6.000000     4.400000      0.000000\n",
      "50%       65.000000      4.00000     7.100000     4.700000      0.000000\n",
      "75%       71.000000     12.00000     8.400000     5.000000      0.000000\n",
      "max       84.000000     20.00000    26.500000     7.600000      3.000000\n",
      "\n",
      "Value Counts for sex:\n",
      " sex\n",
      "male      12328\n",
      "female     3775\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for trx:\n",
      " trx\n",
      "Drug       10727\n",
      "Placebo     5376\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for adverse_effects:\n",
      " adverse_effects\n",
      "No     14567\n",
      "Yes     1536\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "print(\"First 5 rows:\\n\", df.head())\n",
    "\n",
    "# Random Sample\n",
    "print(\"\\nSample of Data:\\n\", df.sample(n=5))\n",
    "\n",
    "# Get a summary of the DataFrame, including data types and non-null counts\n",
    "print(\"\\nDataFrame Info:\\n\", df.info())\n",
    "\n",
    "# Get descriptive statistics for numerical columns\n",
    "print(\"\\nNumerical Statistics:\\n\", df.describe())\n",
    "\n",
    "# Get value counts for categorical columns to understand distributions\n",
    "for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "    print(f\"\\nValue Counts for {col}:\\n\", df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7807770",
   "metadata": {},
   "source": [
    "Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34791de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "print(\"Missing Values:\\n\", df.isna().sum())\n",
    "\n",
    "# Option 1: Impute numerical missing values with the mean or median\n",
    "df['numerical_column'].fillna(df['numerical_column'].mean())\n",
    "\n",
    "# Option 2: Impute string missing values with a custom string\n",
    "df['hair_color'].fillna('baldy')\n",
    "\n",
    "# Option 3: Forward fill missing values (use with caution, especially for time series)\n",
    "df['another_column'].fillna(method='ffill')\n",
    "\n",
    "# Option 4: Backward fill missing values (use with caution)\n",
    "df['yet_another_column'].fillna(method='bfill')\n",
    "\n",
    "# Option 5: Remove entire rows. A: with any missing values or \n",
    "df.dropna()\n",
    "# B: ONLY where ALL cells in the row are NaN\n",
    "df.dropna(how='all')\n",
    "\n",
    "# Option 6: Remove columns with too many missing values (e.g., > 70% missing)\n",
    "missing_percentage = (df.isna().sum() / len(df)) * 100\n",
    "cols_to_drop = missing_percentage[missing_percentage > 70].index\n",
    "# df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Option 7: Flag missing values (create a new binary column)\n",
    "df['numerical_column_missing'] = df['numerical_column'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60227d75",
   "metadata": {},
   "source": [
    "Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicate rows\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# View duplicate rows\n",
    "print(\"\\nDuplicate Rows:\\n\", df[df.duplicated(keep=False)]) # keep=False shows all duplicates\n",
    "\n",
    "# Remove duplicate rows, keeping the first occurrence\n",
    "df.drop_duplicates()\n",
    "\n",
    "# Remove duplicate rows, keeping the last occurrence\n",
    "df.drop_duplicates(keep='last')\n",
    "\n",
    "# Remove duplicates based on a subset of columns\n",
    "df.drop_duplicates(subset=['column1', 'column2'])\n",
    "\n",
    "# Identify and keep only truly unique rows (where no exact duplicate exists anywhere)\n",
    "# Using keep=False ensures that ALL occurrences of a duplicate set are dropped.\n",
    "df_truly_unique = df.drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bedc27",
   "metadata": {},
   "source": [
    "Correct Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc068a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "print(\"\\nCurrent Data Types:\\n\", df.dtypes)\n",
    "\n",
    "# Convert a column to a different data type\n",
    "df['numerical_column'] = pd.to_numeric(df['numerical_column'], errors='coerce') # 'coerce' will turn invalid parsing into NaN\n",
    "df['thousands_seperator'].str.replace(',', '') #replace comma with no space to remove, then covert to_numeric\n",
    "df['date_column'] = pd.to_datetime(df['date_column'], format='%Y-%m-%d', errors='coerce')\n",
    "df['categorical_column'] = df['categorical_column'].astype('category') \n",
    "df['binary_column'] = df['binary_column'].astype('boolean') # Assuming 0/1 or True/False\n",
    "df['adverse_effects'] = df['adverse_effects'].str.lower().str.strip().map({'no': False, 'yes':True}).astype('boolean') # or similar \n",
    "\n",
    "# Convert a column to string\n",
    "df['id_column'] = df['id_column'].astype('string') # StringDtype > 'str' for performance and without converting NaN's to strings\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nUpdated Data Types:\\n\", df.dtypes)\n",
    "\n",
    "# why boolean?\n",
    "# Memory Efficiency: It's highly memory-efficient, storing each value as a single bit (or very close to it) internally.\n",
    "# Clarity and Correctness: It accurately represents the true/false nature of the data, which is what 'yes'/'no' logically imply.\n",
    "# Handles Missing Values: Unlike NumPy's bool dtype, Pandas' boolean dtype gracefully handles missing values (pd.NA). If your data might have blanks, unknown responses, or other indicators of missingness, boolean is crucial because it won't coerce them to True or False.\n",
    "# Direct Logical Operations: You can perform logical operations (&, |, ~) directly on these columns.\n",
    "\n",
    "# why categorical?\n",
    "# Memory Efficiency\n",
    "# Performance Improvement\n",
    "# Statistical Signaling\n",
    "# Optional Defined Order (Ordinal Data): Categorical data can be ordered or unordered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59d397",
   "metadata": {},
   "source": [
    "Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa8303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Data that shouldn't be negative\n",
    "df['numerical_column'] = df['numerical_column'].abs()\n",
    "\n",
    "# For numerical columns, using IQR (Interquartile Range) method\n",
    "Q1 = df['numerical_column'].quantile(0.25)\n",
    "Q3 = df['numerical_column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['numerical_column'] < lower_bound) | (df['numerical_column'] > upper_bound)]\n",
    "print(\"\\nOutliers in numerical_column:\\n\", outliers)\n",
    "\n",
    "sns.catplot(kind='box', data=df, y='numerical_column') \n",
    "plt.title('numerical_column')\n",
    "plt.show()\n",
    "\n",
    "# Option 1: Remove outliers/ data that falls outside of a desired min & max\n",
    "df_no_outliers = df[~((df['numerical_column'] < lower_bound) | (df['numerical_column'] > upper_bound))]\n",
    "\n",
    "# Option 2: Cap or floor outliers, replacing outliers with the lower/upper bound value\n",
    "df['numerical_column_capped'] = df['numerical_column'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Option 3: Transform the data (e.g., log transformation for skewed data)\n",
    "import numpy as np\n",
    "# df['numerical_column_log'] = np.log(df['numerical_column']) # Be mindful of zero or negative values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648c62f",
   "metadata": {},
   "source": [
    "Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4487bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert case\n",
    "df['text_column'] = df['text_column'].str.lower()\n",
    "df['text_column'] = df['text_column'].str.upper()\n",
    "df['text_column'] = df['text_column'].str.title()\n",
    "\n",
    "# Remove leading/trailing whitespace\n",
    "df['text_column'] = df['text_column'].str.strip()\n",
    "\n",
    "# Remove punctuation\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "df['text_column'] = df['text_column'].apply(remove_punctuation)\n",
    "\n",
    "# Replace entire matching cell value, regardless of data type\n",
    "df['column'] = df['column'].replace('a', 'b')\n",
    "\n",
    "# Remove special characters\n",
    "df['text_column'] = df['text_column'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "\n",
    "# Correct spelling errors (can be complex, often involves libraries like `fuzzywuzzy` or manual mapping)\n",
    "# Example of simple replacement:\n",
    "df['text_column'] = df['text_column'].str.replace('misspelled', 'correct')\n",
    "\n",
    "# Handle inconsistent formatting (e.g., different ways of writing the same thing)\n",
    "df['city_column'] = df['city_column'].str.replace('St.', 'Saint', regex=False).str.strip()\n",
    "\n",
    "# Split the 'Name' column into 'First Name' and 'Last Name'\n",
    "df[['First Name', 'Last Name']] = df['Name'].str.split(',', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940029a8",
   "metadata": {},
   "source": [
    "Date & Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc33c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle strings or different date formats during conversion\n",
    "df['date_column'] = pd.to_datetime(df['date_column'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Extract date components\n",
    "df['year'] = df['date_column'].dt.year\n",
    "df['month'] = df['date_column'].dt.month\n",
    "df['day'] = df['date_column'].dt.day\n",
    "df['day_of_week'] = df['date_column'].dt.day_name()\n",
    "\n",
    "# Handle time components if present\n",
    "df['hour'] = df['date_column'].dt.hour\n",
    "df['minute'] = df['date_column'].dt.minute\n",
    "df['second'] = df['date_column'].dt.second\n",
    "\n",
    "# Handle time zones (if applicable)\n",
    "# df['datetime_utc'] = df['datetime_column'].dt.tz_localize('UTC')\n",
    "# df['datetime_local'] = df['datetime_utc'].dt.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aced5a",
   "metadata": {},
   "source": [
    "Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2979fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distinct values for inconsistencies, use .value_counts for how often they appear\n",
    "print(\"\\nDistinct values in categorical_column:\\n\", df['categorical_column'].unique())\n",
    "\n",
    "# Defining categories\n",
    "df = pd.DataFrame({\n",
    "    'fruit': ['apple', 'banana', 'orange', 'apple', 'banana'],\n",
    "    'size': ['small', 'large', 'medium', 'small', 'large']})\n",
    "df['fruit'] = df['fruit'].astype('category')\n",
    "df['size'] = df['size'].astype('category')\n",
    "\n",
    "# You can also specify it during Series creation\n",
    "s_cat = pd.Series(['A', 'B', 'A', 'C'], dtype='category')\n",
    "\n",
    "# Standardize categories (e.g., 'USA', 'U.S.A.', 'United States' to 'USA')\n",
    "df['country_column'] = df['country_column'].replace(['U.S.A.', 'United States'], 'USA')\n",
    "\n",
    "# Group less frequent categories into an 'Other' category (to reduce dimensionality)\n",
    "value_counts = df['category_column'].value_counts()\n",
    "value_counts_proportion = df['category_column'].value_counts(normalize=True)\n",
    "infrequent_categories = value_counts[value_counts < 10].index # Example threshold\n",
    "df['category_cleaned'] = df['category_column'].apply(lambda x: 'Other' if x in infrequent_categories else x)\n",
    "\n",
    "# Create categorical data type column\n",
    "df['categorical_column'] = df['categorical_column'].astype('category') \n",
    "\n",
    "# why categorical?\n",
    "# Memory Efficiency\n",
    "# Performance Improvement\n",
    "# Statistical Signaling\n",
    "# Optional Defined Order (Ordinal Data): Categorical data can be ordered or unordered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a13706",
   "metadata": {},
   "source": [
    "Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba42855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run descriptive statistics and info\n",
    "print(\"\\nCleaned DataFrame Info:\\n\", df.info())\n",
    "print(\"\\nCleaned Numerical Statistics:\\n\", df.describe())\n",
    "\n",
    "# Check for remaining missing values or duplicates\n",
    "print(\"\\nMissing Values After Cleaning:\\n\", df.isna().sum())\n",
    "print(\"\\nNumber of Duplicate Rows After Cleaning:\", df.duplicated().sum())\n",
    "\n",
    "# Sample the cleaned data to visually inspect\n",
    "print(\"\\nSample of Cleaned Data:\\n\", df.sample(5))\n",
    "\n",
    "# Uniqueness check for categorical columns \n",
    "for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "    print(f\"\\nValue Counts for {col}:\\n\", df[col].value_counts(dropna=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
